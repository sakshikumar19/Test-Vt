DETAILED QUERY LOG - 2025-03-15 22:38:42
======================================================================

QUESTION: How can I debug elevated query latency on a Vitess primary?

ANSWER: ### Debugging Elevated Query Latency on a Vitess Primary
To debug elevated query latency on a Vitess primary, follow these steps:

1. **Inspect QPS graphs**: Check if the queries per second (QPS) have increased. If yes, drill down into more detailed QPS graphs to identify which table or user is causing the increase.
2. **Analyze queries**: If a specific table is identified, use `/debug/queryz` to analyze queries on that table.
3. **Take action**: Inform the engineer about toxic queries. If it's a specific user, consider stopping their job or throttling them. As a last resort, blacklist the query to maintain system health.

If QPS has not increased, but latency has:

* **Inspect per-table latency graphs**: Identify if the issue is specific to a table. If so, it may be a long-running, low QPS query skewing the numbers.
* **Optimize the culprit query**: Take necessary steps to optimize the query.

If latency is up across the board:

* **Inspect transaction latency**: Check if transaction latency has increased. If so, investigate if MySQL is running too many concurrent transactions.
* **Check for tx pool full errors**: Look for errors in the INFO logs, which may indicate a specific sequence of statements causing the issue.
* **Investigate root cause**: Determine if the issue is due to network problems or recent changes in app behavior.

If none of the above steps identify the issue:

* **Check system variables**: Inspect hardware faults, disk latency, and memory parity errors.
* **Consider failover**: If a hardware issue is found, consider failing over to a new machine.

By following these steps, you can methodically debug and address elevated query latency on a Vitess primary.

METRICS:
  - Documents Retrieved: 4
  - Top Document Score: 0.9988910555839539
  - Response Time: 20245.7 ms

RETRIEVED DOCUMENTS:
Document 1:
  Source: Unknown
  Score: 0.9988910555839539
  Content:
----------------------------------------
--- title: Elevated query latency on primary description: Debug common issues with Vitess weight: 1 --- ## Elevated query latency on primary Diagnosis 1: Inspect the graphs to see if QPS has gone up. If yes, drill down on the more detailed QPS graphs to see which table, or user caused the increase. If a table is identified, look at /debug/queryz for queries on that table. Action: Inform engineer about the toxic queries. If it’s a specific user, you can stop their job or throttle them to keep the load manageable. As a last resort, blacklist query to allow the rest of the system to stay healthy. Diagnosis 2: QPS did not go up, only latency did. Inspect the per-table latency graphs. If it’s a specific table, then it’s most likely a long-running low QPS query that’s skewing the numbers. Identify the culprit query and take necessary steps to get it optimized. Such queries usually do not cause outage. So, there may not be a need to take extreme measures. Diagnosis 3: Latency seems to be up across the board. Inspect transaction latency. If this has gone up, then something is causing MySQL to run too many concurrent transactions which causes slow-down. See if there are any tx pool full errors. If there is an increase, the INFO logs will dump info about all transactions. From there, you should be able to if a specific sequence of statements is causing the problem. Once that is identified, find out the root cause. It could be network issues, or it could be a recent change in app behavior. Diagnosis 4: No particular transaction seems to be the culprit. Nothing seems to have changed in any of the requests. Look at system variables to see if there are hardware faults. Is the disk latency too high? Are there memory parity errors? If so, you may have to failover to a new machine. --- title: Primary starts up read-only description: Debug common issues with Vitess weight: 5 --- ## Primary starts up read-only To prevent accidentally accepting writes, our default my.cnf settings tell MySQL to always start up read-only. If the primary MySQL gets restarted, it will thus come back read-only until someone intervene to confirm that it should accept writes. If VTOrc is running, then it will take care of converting the primary to read-write mode. However, to fix manually, you can use the [`SetReadWrite`](../../reference/programs/vtctl/tablets/#setreadwrite) command to do that. Usually if something unexpected happens to the primary, it's better to reparent to a different replica with [`EmergencyReparentShard`](../../reference/programs/vtctl/shards/#emergencyreparentshard). If you need to do planned maintenance on the primary, it's best to first reparent to another replica with [`PlannedReparentShard`](../../reference/programs/vtctl/shards/#plannedreparentshard). --- title: Vitess sees the wrong tablet as primary description: Debug common issues with Vitess weight: 10 --- ## Vitess sees the wrong tablet as primary If you do a failover manually (not through Vitess), you'll need to tell Vitess which tablet corresponds to the new primary MySQL. Until then, writes will fail since they'll be 
----------------------------------------

Document 2:
  Source: Unknown
  Score: 0.921425998210907
  Content:
----------------------------------------
specific query is causing the problem. There are two approaches: * Inspect the `/queryz` page and look at the stats for all queries of that table. It is very likely that the problematic ones have already risen to the top of the page and may be color-coded red. If not, talking a few snapshots of the page and comparing the stats should help identify the problematic query. * Real-time stream from `/debug/querylog`, filtering out unwanted tables and observing the results to identify the problematic query. Once the query is identified, the remedy depends on the situation. It could be a rewrite of the query to be more efficient, the creation of an additional index, or it could be the shutdown of an abusive batch process. The above guidelines have not required you to inspect MySQL. Over time, Vitess has evolved by improving its observability every time there was an incident. However, there may still be situations where the above approach is insufficient. If so, you will need to resort to looking inside MySQL to find out the root cause. The actual identification of the root cause may not be as straightforward as described above. Sometimes, an incident is caused by multiple factors. In such cases, using first principles of troubleshooting and understanding how the components communicate with each other may be the only way to get to the bottom of a problem. If you have exhausted all the recommendations given so far and still have not found the root cause, you may have to directly troubleshoot the problem at the MySQL level. ### Elevated Error Rates The analysis for elevated error rates for read queries follows steps similar to elevated latency. You should essentially use the same drill down approach to identify the root cause. ### Transaction timeouts Transaction timeouts manifest as the following errors: ```text ERROR 1317 (HY000): vtgate: http://sougou-lap1:15001/: vttablet: rpc error: code = Aborted desc = transaction 1610909864463057369: ended at 2021-01-17 10:58:49.155 PST (exceeded timeout: 30s) (CallerID: userData1) ``` If you see such errors, you may have to do one of the following: * Increase the transaction timeout in vttablet by setting a higher value for `queryserver-config-transaction-timeout`. * Refactor the application code to finish the transaction sooner. It is recommended to minimize long running transactions in MySQL. This is because the efficiency of MySQL drastically drops as the number of concurrent transactions increases. ### Transaction connection limit errors If you encounter errors that contain the following text: `transaction pool connection limit exceeded`, it means that your connection pool for transactions is full and Vitess timed out waiting for a connection. This issue can have multiple root causes. If your transaction load is just spiky, then you may just have to increase the pool timeout to make the transaction wait longer for a connection. This can be increased by setting the `queryserver-config-txpool-timeout` flag in vttablet. The default value is one second. It is also possible that you have underprovisioned the transaction pool size. If so, you can increase 
----------------------------------------

Document 3:
  Source: Unknown
  Score: 0.6473439335823059
  Content:
----------------------------------------
break up of all queries by command, keyspace, and type. #### HealthcheckConnections It shows the number of tablet connections for query/healthcheck per keyspace, shard, and tablet type. #### TopologyWatcherErrors and TopologyWatcherOperations These two variables track events related to how vtgate watches the topology. It is particularly important to monitor the error count. This can act as an early warning sign if a vtgate is not able to refresh the list of tablets from the topo. #### VindexUnknownParameters Gauges the number of unknown Vindex params in the latest VSchema obtained from the topology. ### /debug/health This URL prints out a simple "ok" or “not ok” string that can be used to check if the server is healthy. ### /debug/querylogz, /debug/querylog, /debug/queryz, /debug/query\_plans These URLs are similar to the ones exported by vttablet, but apply to the current vtgate instance. ### /debug/vschema This URL shows the vschema as loaded by vtgate. ### Alerting For vtgate, here’s a list of possible variables to alert on: * Error rate * Error/query rate * Error/query/tablet-type rate * vtgate serving graph is stale by x minutes (topology service is down) * QPS/core * Latency --- title: Planning weight: 3 aliases: ['/docs/user-guides/configuration-basic/configuring-components/'] --- This guide explains how to bring up and manage a Vitess cluster. We cover every individual component of Vitess and how they interact with each other. If you are deploying on Kubernetes, a lot of the wire-up is automatically handled by the operator. However, it is still important to know how the components work in order to be able to troubleshoot problems if they occur in production. We assume that you are familiar with the setup of your production environment. If operating in Kubernetes, you should be able to access all the logs, and be able to reach any ports of the pods that are getting launched. In addition, you should be familiar with provisioning storage. In self-hosted environments, you are expected to have the ability to troubleshoot network issues, firewalls and hostnames. You will also have to configure and setup certificates for components to talk to each other securely. This topic will not be covered in this guide. In addition, you are expected to perform all other sysadmin work related to provisioning, resource allocation, etc. Vitess is capable of running on a variety of platforms. They may be self-hosted, in the public cloud, or in a cloud orchestration environment like Kubernetes. In this guide, we will assume that we are deploying in a self-hosted environment that has multiple data centers. This setup allows us to better understand the interaction between the components. Before starting, we assume that you have downloaded Vitess and finished the [Get Started](../../../get-started) tutorial. ## External tools Vitess relies on two external components, and we recommend that you choose them upfront: 1. [TopoServer](../../../concepts/topology-service/): This is the server in which Vitess stores its metadata. We recommend etcd if you have no other preference. 2. [MySQL](../../../overview/supported-databases/): Vitess supports MySQL/Percona Server 5.7 to 8.0. We recommend MySQL 8.0 for new installations. In this 
----------------------------------------

Document 4:
  Source: Unknown
  Score: 0.6393367648124695
  Content:
----------------------------------------
usually running on the same machine. Each tablet is assigned a tablet type, which specifies what role it currently performs. The main tablet types are listed below: * PRIMARY - A tablet that contains a MySQL instance that is currently the MySQL primary for its shard. * REPLICA - A tablet that contains a MySQL replica that is eligible to be promoted to primary. Conventionally, these are reserved for serving live, user-facing read-only requests (like from the website’s frontend). * RDONLY - A tablet that contains a MySQL replica that cannot be promoted to primary. Conventionally, these are used for background processing jobs, such as taking backups, dumping data to other systems, heavy analytical queries, and resharding. There are some other tablet types like `BACKUP` and `RESTORE`. For information on how to use tablets please review this [user guide](https://vitess.io/docs/user-guides/configuration-basic/vttablet-mysql/). ## What is a shard? A shard is a physical division within a keyspace; i.e. how data is split across multiple MySQL instances. A shard typically consists of one MySQL primary and one or more MySQL replicas. Each MySQL instance within a shard has the same data, if the effect of MySQL replication lag is ignored. The replicas can serve read-only traffic, execute long-running queries from data analysis tools, or perform administrative tasks. An unsharded keyspace always has only a single shard. --- title: Metrics weight: 7 --- ## How can I monitor or get metrics from Vitess? All Vitess components have a web UI that you can access to see the state of each component. The first place to look is the `/debug/status` page. * This is the main landing page for a VTGate, which displays the status of a particular server. A list of tablets this VTGate process is connected to is also displayed, as this is the list of tablets that can potentially serve queries. A second place to look is the `/debug/vars` page. For example, for VTGate, this page contains the following items: * VTGateApi - This is the main histogram variable to track for VTGates. It gives you a breakdown of all queries by command, keyspace, and type. * HealthcheckConnections - It shows the number of tablet connections for query/healthcheck per keyspace, shard, and tablet type. There are two other pages you can use to get monitoring information from Vitess in the VTGate web UI: * `/debug/query_plans` - This URL gives you all the query plans for queries going through VTGate. * `/debug/vschema` - This URL shows the VSchema as loaded by VTGate. VTTablet has a similar web UI. Vitess component metrics can also be scraped via /metrics. This will provide a Prometheus-format metric dump that is updated continuously. This is the recommended way to collect metrics from Vitess. ## How do you integrate Prometheus and Vitess? There is an Prometheus exporter that is on by default that enables you to configure a Prometheus compatible scraper to grab data from the various Vitess components. All Vitess components export their metrics on their http port at `/metrics`. 
----------------------------------------

