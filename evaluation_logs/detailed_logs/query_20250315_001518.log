DETAILED QUERY LOG - 2025-03-15 00:15:18
======================================================================

QUESTION: How do I debug elevated query latency on a Vitess primary?

ANSWER: ### Debugging Elevated Query Latency on a Vitess Primary
To debug elevated query latency on a Vitess primary, follow these steps:

1. **Inspect QPS Graphs**: Check if the queries per second (QPS) have increased. If yes, drill down into more detailed QPS graphs to identify which table or user is causing the increase.
2. **Analyze Queryz**: If a specific table is identified, look at `/debug/queryz` for queries on that table to find toxic queries.
3. **Inform Engineer and Take Action**: Inform the engineer about the toxic queries. If it's a specific user, consider stopping their job or throttling them to manage the load. As a last resort, blacklist the query to keep the system healthy.

If QPS has not increased, but latency has:

* **Inspect Per-Table Latency Graphs**: Identify if the issue is specific to a table. If so, it's likely a long-running, low QPS query skewing the numbers.
* **Optimize the Culprit Query**: Identify and optimize the problematic query.

If latency is up across the board:

* **Inspect Transaction Latency**: Check if transaction latency has increased. If so, MySQL might be running too many concurrent transactions, causing a slowdown.
* **Check for Tx Pool Full Errors**: Look for transaction pool full errors in the INFO logs, which can provide information about problematic transactions.
* **Identify the Root Cause**: Determine the root cause, which could be network issues or recent changes in application behavior.

If none of these steps identify the issue:

* **Check System Variables**: Look for hardware faults, such as disk issues.
* **Inspect MySQL (if necessary)**: As a last resort, inspect MySQL to find the root cause, which may involve looking at query logs or system variables.

METRICS:
  - Documents Retrieved: 3
  - Top Document Score: 0.9993916749954224
  - Response Time: 18353.56 ms
  - Retrieval Time (est.): 12847.49 ms

RETRIEVED DOCUMENTS:
Document 1:
  Source: Unknown
  Score: 0.9993916749954224
  Content:
----------------------------------------
--- title: Elevated query latency on primary description: Debug common issues with Vitess weight: 1 --- ## Elevated query latency on primary Diagnosis 1: Inspect the graphs to see if QPS has gone up. If yes, drill down on the more detailed QPS graphs to see which table, or user caused the increase. If a table is identified, look at /debug/queryz for queries on that table. Action: Inform engineer about the toxic queries. If it’s a specific user, you can stop their job or throttle them to keep the load manageable. As a last resort, blacklist query to allow the rest of the system to stay healthy. Diagnosis 2: QPS did not go up, only latency did. Inspect the per-table latency graphs. If it’s a specific table, then it’s most likely a long-running low QPS query that’s skewing the numbers. Identify the culprit query and take necessary steps to get it optimized. Such queries usually do not cause outage. So, there may not be a need to take extreme measures. Diagnosis 3: Latency seems to be up across the board. Inspect transaction latency. If this has gone up, then something is causing MySQL to run too many concurrent transactions which causes slow-down. See if there are any tx pool full errors. If there is an increase, the INFO logs will dump info about all transactions. From there, you should be able to if a specific sequence of statements is causing the problem. Once that is identified, find out the root cause. It could be network issues, or it could be a recent change in app behavior. Diagnosis 4: No particular transaction seems to be the culprit. Nothing seems to have changed in any of the requests. Look at system variables to see if there are hardware faults. Is the disk 
----------------------------------------

Document 2:
  Source: Unknown
  Score: 0.9651079177856445
  Content:
----------------------------------------
specific query is causing the problem. There are two approaches: * Inspect the `/queryz` page and look at the stats for all queries of that table. It is very likely that the problematic ones have already risen to the top of the page and may be color-coded red. If not, talking a few snapshots of the page and comparing the stats should help identify the problematic query. * Real-time stream from `/debug/querylog`, filtering out unwanted tables and observing the results to identify the problematic query. Once the query is identified, the remedy depends on the situation. It could be a rewrite of the query to be more efficient, the creation of an additional index, or it could be the shutdown of an abusive batch process. The above guidelines have not required you to inspect MySQL. Over time, Vitess has evolved by improving its observability every time there was an incident. However, there may still be situations where the above approach is insufficient. If so, you will need to resort to looking inside MySQL to find out the root cause. The actual identification of the root cause may not be as straightforward as described above. Sometimes, an incident is caused by multiple factors. In such cases, using first principles of troubleshooting and understanding how the components communicate with each other may be the only way to get to the bottom of a problem. If you have exhausted all the recommendations given so far and still have not found the root cause, you may have to directly troubleshoot the problem at the MySQL level. ### Elevated Error Rates The analysis for elevated error rates for read queries follows steps similar to elevated latency. You should essentially use the same drill down approach to identify the root cause. ### Transaction timeouts Transaction timeouts 
----------------------------------------

Document 3:
  Source: Unknown
  Score: 0.8094999194145203
  Content:
----------------------------------------
look to see if a specific query is the root cause. This is list is sorted in descending order of query latency. If the value is greater than 100 milliseconds, it's color-coded red. If it is greater than 10 milliseconds, it is color coded yellow. Otherwise, it is color coded gray. * /debug/query\_stats is a JSON view of the per-query stats. This information is pulled in real-time from the query cache. The per-table stats in /debug/vars are a roll-up of this information. * /debug/tablet\_plans is a more static view of the query cache. It just shows how vttablet will process or rewrite the input query. * /livequeryz lists the currently running queries. You have the option to kill any of them from this page. #### /querylogz, /debug/querylog, /txlogz, /debug/txlog * /debug/querylog is a continuous stream of verbose execution info as each query is executed. This URL can generate a lot of data because it streams every query processed by vttablet. The details are as per this function: https://github.com/vitessio/vitess/blob/main/go/vt/vttablet/tabletserver/tabletenv/logstats.go#L179 * /querylogz is a limited human readable version of /debug/querylog. It prints the next 300 queries by default. The limit can be specified with a limit=N parameter on the URL. * /txlogz is like /querylogz, but for transactions. * /debug/txlog is the JSON counterpart to /txlogz. #### /debug/consolidations This URL has an MRU list of consolidations. This is a way of identifying if multiple clients are spamming the same query to a server. #### /schemaz, /debug/schema * /schemaz shows the schema info loaded by vttablet. * /debug/schema is the JSON version of /schemaz. #### /debug/query\_rules This URL displays the currently active query blacklist rules. ### Alerting Alerting is built on top of the variables you monitor. Before setting up alerts, you should get some baseline stats and variance, and then you can 
----------------------------------------

