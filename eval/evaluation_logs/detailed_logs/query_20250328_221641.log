DETAILED QUERY LOG - 2025-03-28 22:16:41
======================================================================

QUESTION: I am running with the semi_sync durability policy. How many nodes have to crash for it to require manual intervention?

ANSWER: ### Semi-Sync Durability Policy
With the semi-sync durability policy, at least one replica must acknowledge a write before it is considered committed. 

If you are running with this policy, **it will require manual intervention if the primary and at least one replica crash**. This is because the semi-sync policy requires at least one replica to acknowledge a write, so if all replicas crash, there is no way for VTOrc to automatically recover and ensure data consistency.

In other words, if the primary and all replicas crash, VTOrc will not be able to recover automatically, and manual intervention will be required to restore the cluster.

METRICS:
  - Documents Retrieved: 3
  - Top Document Score: 0.2521042048931122
  - Response Time: 22176.18 ms
  - Retrieval Time (est.): 15523.33 ms

RETRIEVED DOCUMENTS:
Document 1:
  Source: Unknown
  Score: 0.2521042048931122
  Content:
----------------------------------------
\ --instance-poll-time "1s" \ --topo-information-refresh-duration "30s" \ --alsologtostderr ``` You can optionally add a `clusters_to_watch` flag that contains a comma separated list of keyspaces or `keyspace/shard` values. If specified, VTOrc will manage only those clusters. ### Durability Policies All the failovers that VTOrc performs will be honoring the [durability policies](../../configuration-basic/durability_policy). Please be careful in setting the desired durability policies for your keyspace because this will affect what situations VTOrc can recover from and what situations will require manual intervention. ### Running VTOrc using the Vitess Operator To find information about deploying VTOrc using Vitess Operator please take a look at this [page](../../../reference/vtorc/running_with_vtop). --- title: VTTablet and MySQL weight: 8 --- Let us assume that we want to bring up a single unsharded keyspace. The first step is to identify the number of replicas (including the primary) we would like to deploy. We should also make a decision about how to distribute them across the cells. Vitess requires you to assign a globally unique id (tablet UID) to every vttablet. This has to be an unsigned 32-bit integer. This is a legacy requirement derived from the fact that the MySQL server id (also an unsigned 32-bit integer) used to be the same as the tablet uid. This is not the case any more. In terms of mapping these components to machines, Vitess allows you to run multiple of these on the same machine. If this is the case, you will need to assign non-conflicting ports for these servers to listen on. VTTablet and MySQL are meant to be brought up as a pair within the same machine. By default, vttablet will connect to its MySQL over a unix socket. Let us look at the steps to bring up the first pair for an unsharded keyspace `commerce` in cell1 and a tablet 
----------------------------------------

Document 2:
  Source: Unknown
  Score: 0.10342297554016112
  Content:
----------------------------------------
### Options inherited from parent commands ``` --action_timeout duration timeout to use for the command (default 1h0m0s) --compact use compact format for otherwise verbose outputs --server string server to use for the connection (required) --topo-global-root string the path of the global topology data in the global topology server (default "/vitess/global") --topo-global-server-address strings the address of the global topology server(s) (default [localhost:2379]) --topo-implementation string the topology implementation to use (default "etcd2") ``` ### SEE ALSO * [vtctldclient](../) - Executes a cluster management command on the remote vtctld server or alternatively as a standalone binary using --server=internal. --- title: CreateKeyspace series: vtctldclient --- ## vtctldclient CreateKeyspace Creates the specified keyspace in the topology. ### Synopsis Creates the specified keyspace in the topology. For a SNAPSHOT keyspace, the request must specify the name of a base keyspace, as well as a snapshot time. ``` vtctldclient CreateKeyspace <keyspace> [--force|-f] [--type KEYSPACE_TYPE] [--base-keyspace KEYSPACE --snapshot-timestamp TIME] [--served-from DB_TYPE:KEYSPACE ...] [--durability-policy <policy_name>] [--sidecar-db-name <db_name>] ``` ### Options ``` -e, --allow-empty-vschema Allows a new keyspace to have no vschema. --base-keyspace string The base keyspace for a snapshot keyspace. --durability-policy string Type of durability to enforce for this keyspace. Default is none. Possible values include 'semi_sync' and others as dictated by registered plugins. (default "none") -f, --force Proceeds even if the keyspace already exists. Does not overwrite the existing keyspace record. -h, --help help for CreateKeyspace --sidecar-db-name string (Experimental) Name of the Vitess sidecar database that tablets in this keyspace will use for internal metadata. (default "_vt") --snapshot-timestamp string The snapshot time for a snapshot keyspace, as a timestamp in RFC3339 format. --type cli.KeyspaceTypeFlag The type of the keyspace. (default NORMAL) ``` ### Options inherited from parent commands ``` --action_timeout duration timeout to use for the command (default 1h0m0s) --compact use compact format for otherwise verbose outputs --server 
----------------------------------------

Document 3:
  Source: Unknown
  Score: 0.09445703029632568
  Content:
----------------------------------------
shards are deleted, you can delete the keyspace with: ```text vtctldclient DeleteKeyspace commerce ``` `DeleteKeyspace` also supports a `--recursive` flag that loops through all shards and deletes them recursively, but also ignores any tablets that are running. Note that restarting a vttablet for the deleted keyspace will cause the keyspace to be recreated. This is yet another reason for ensuring that all vttablets are shutdown upfront. --- title: Durability Policy weight: 10 --- Vitess now supports a configurable interface for durability policies. Users can now define, in the interface, which tablets are eligible to be promoted to a PRIMARY instance. They can also specify the number of semi-sync acknowledgements it requires and the tablets which are eligible to send these acknowledgements. The interface definition looks like: ```go // Durabler is the interface which is used to get the promotion rules for candidates and the semi sync setup type Durabler interface { promotionRule(*topodatapb.Tablet) promotionrule.CandidatePromotionRule semiSyncAckers(*topodatapb.Tablet) int isReplicaSemiSync(primary, replica *topodatapb.Tablet) bool } ``` There are 3 implementations supported in this release: - ***semi_sync*** - This durability policy sets the number of required semi-sync acknowledgements to 1. It only allows REPLICA type tablets to send semi-sync acknowledgements. It returns NeutralPromoteRule for REPLICA tablet types, MustNotPromoteRule for everything else. - ***semi_sync_with_rdonly_ack*** - This durability policy sets the number of required semi-sync acknowledgements to 1. It allows REPLICA and RDONLY type tablets to acknowledge semi sync. It returns NeutralPromoteRule for REPLICA tablet types, MustNotPromoteRule for everything else. - ***none** (default)* - This durability policy does not set any semi-sync configuration. It returns NeutralPromoteRule for REPLICA tablet types, MustNotPromoteRule for everything else - ***cross_cell*** - This durability policy sets the number of required semi-sync acknowledgements to 1. It only allows REPLICA type tablets from a different cell than the current primary to send semi-sync acknowledgements. It 
----------------------------------------

