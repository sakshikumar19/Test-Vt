DETAILED QUERY LOG - 2025-03-28 22:16:19
======================================================================

QUESTION: Is there a metric in VTOrc to tell me how many recoveries are currently running?

ANSWER: ### Checking Recoveries in VTOrc
Yes, there is a metric in VTOrc to tell you how many recoveries are currently running. 

The metric is called `PendingRecoveries`, which represents the number of recoveries in progress that haven't completed. You can view this metric on the `/debug/vars` page.

METRICS:
  - Documents Retrieved: 3
  - Top Document Score: 1.5240305471420288
  - Response Time: 16424.62 ms
  - Retrieval Time (est.): 11497.23 ms

RETRIEVED DOCUMENTS:
Document 1:
  Source: Unknown
  Score: 1.5240305471420288
  Content:
----------------------------------------
range from replication not running to errant GTIDs. The new API also supports filtering using the keyspace and shard name | | `/api/disable-global-recoveries` | This API disables the global recoveries in VTOrc. This makes it so that VTOrc doesn't repair any failures it detects. | | `/api/enable-global-recoveries` | This API enables the global recoveries in VTOrc. | | `/debug/health` | This API outputs the health of the VTOrc process. | | `/debug/liveness` | This API outputs the liveness of the VTOrc process. | | `/api/replication-analysis` | This API shows the replication analysis of VTOrc. Output is in JSON format. | | `/api/errant-gtids` | This API shows the tablets that have errant GTIDs as detected by VTOrc. Output is in JSON format. This API supports filtering by keyspace and shard name. | | `/api/database-state` | This API shows the internal database state of VTOrc. This API can be used to debug VTOrc. | # Metrics Metrics are available to be seen on the `/debug/vars` page. VTOrc exports the following metrics: | Metric | Usage | |------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------| | `PendingRecoveries` | The number of recoveries in progress which haven't completed. | | `RecoveriesCount` | The number of recoveries run. This is further subdivided for all the different recoveries. | | `SuccessfulRecoveries` | The number of succesful recoveries run. This is further subdivided for all the different recoveries. | | `FailedRecoveries` | The number of recoveries that failed. This is further subdivided for all the different recoveries. | | `ErrantGtidTabletCount` | The number of tablets with errant GTIDs as detected by VTOrc. | | `DetectedProblems` | Binary gauge that shows the active problems that VTOrc has detected. This is further subdivided by TabletAlias, Keyspace, and Shard. | | `planned_reparent_counts` | Number of times Planned Reparent Shard has been run. It is further subdivided by 
----------------------------------------

Document 2:
  Source: Unknown
  Score: 1.019097397327423
  Content:
----------------------------------------
Running with Vitess Operator description: How to configure Vitess Kubernetes Operator to run VTOrc --- ## Get Started The Vitess operator deploys one VTOrc instance for each keyspace that it is configured for. Please look at the [VTOrc reference page](../../programs/vtorc) to know all the flags that VTOrc accepts. ## Configuring VTOrc in Vitess Operator The VTOrc can be configured to run for a given keyspace by specifying the `vitessOrchestrator` specification as part of the `keyspace` spec. Resource limits and requests can be specified as part of the configuration and the default behaviour of VTOrc can be changed by specifying any desired flags in the `extraFlags` field. The VTOrc UI runs on the port `15000` of the container and port-forwarding can be setup to access it. ## Example Configuration An example deployment from the VTOrc [end to end test](https://github.com/planetscale/vitess-operator/tree/main/test/endtoend) on the Vitess Operator looks like: ```yaml keyspaces: - name: commerce durabilityPolicy: semi_sync turndownPolicy: Immediate vitessOrchestrator: resources: limits: memory: 128Mi requests: cpu: 100m memory: 128Mi extraFlags: recovery-period-block-duration: 5s ``` The full configuration file is available [here](https://github.com/planetscale/vitess-operator/blob/main/test/endtoend/operator/101_initial_cluster_vtorc_vtadmin.yaml). --- title: UI, API and Metrics --- # UI In order to use UI, `--port` flag has to be provided. Currently, the `/debug/status` lists the recent recoveries that VTOrc has performed. ![VTOrc-recent-recoveries](../img/VTOrc-Recent-Recoveries.png) # APIs VTOrc supports the following APIs which can be used for monitoring and changing the behaviour of VTOrc. | New API | Additional notes | |----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | `/api/problems` | This API lists all the instances that have any problems in them. The problems range from replication not running to errant GTIDs. The new API also supports filtering using the keyspace and shard name | | `/api/disable-global-recoveries` | This API disables the global recoveries in VTOrc. This makes it so that VTOrc doesn't repair any failures it detects. | | `/api/enable-global-recoveries` | This API 
----------------------------------------

Document 3:
  Source: Unknown
  Score: 0.903485004901886
  Content:
----------------------------------------
- These are then fixed by issuing RPCs to the associated `vttablets` ```mermaid stateDiagram-v2 start: Collect Information topoServer: Topology Server vttablets: Vttablets infoCollected: Information Received problems: Problems Found fixes: Run Fixes start --> topoServer: Every <code>topo-information-refresh-duration</code> start --> vttablets: Every <code>instance-poll-time</code> topoServer --> infoCollected: Keyspace and Vttablet records vttablets --> infoCollected: MySQL information infoCollected --> problems: Analyze collected information problems --> fixes: RPCs to Vttablets ``` # Coordination among VTOrc instances and `vtctld` Users are encouraged to run multiple instances of VTOrc monitoring the same cluster because VTOrc too, like any other service is liable to failure for reasons out of its control. Also, users run `vtctld` instances which can be used to run commands which alter the desired topology ([PlannedReparentShard](../../../user-guides/configuration-advanced/reparenting/#plannedreparentshard-planned-reparenting)) and durability requirements ([SetKeyspaceDurabilityPolicy](../../programs/vtctldclient/vtctldclient_setkeyspacedurabilitypolicy/)). The most logical question that arises is how do we ensure coordination between multiple VTOrc instances and `vtctld`. We do so by using the existing central topology-server. Each of these services, acquire a shard lock before it proceeds to run any fixes. This ensures that there is only one actor at any given point of time trying to alter the cluster. Another cause of concern could be recoveries run on stale data collected by VTOrc. Since VTOrc instances use a polling method to load the information they use for fault detection, they can sometimes read outdated information. To prevent VTOrc instances from running incorrect/unnecessary recoveries, all VTOrc instances refresh their local information that they require for the fix after acquiring a shard lock. --- title: Running with Vitess Operator description: How to configure Vitess Kubernetes Operator to run VTOrc --- ## Get Started The Vitess operator deploys one VTOrc instance for each keyspace that it is configured for. Please look at the [VTOrc reference page](../../programs/vtorc) to know all the flags that VTOrc accepts. ## Configuring 
----------------------------------------

